{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1wVjFmlPgSRm/IzDkQbPN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadwin-357/GCN/blob/main/Scaling_GNNs_working_with_large_graphes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4I-PxlMMffZ",
        "outputId": "fc5c77e8-7bbd-4a20-a752-78dd2765b7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for a larege graph, we have to partition it into many clusters\n",
        "# based on Cluster-GCN\n",
        "# examply on PubMed citation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "dataset = Planetoid(root='data/Planeoid', name='PubMed', transform=NormalizeFeatures())\n",
        "\n",
        "print(f'dataset: {dataset}')\n",
        "print(f'num of graphes: {len(dataset)}')\n",
        "print(f'num of node features:{dataset.num_features}')\n",
        "print(f'num of classes:{dataset.num_classes}')\n",
        "\n",
        "# the first data\n",
        "data = dataset[0]\n",
        "\n",
        "print(f'data: {data}')\n",
        "print(f'Num of Nodes:{data.num_nodes}')\n",
        "print(f'Num of edges:{data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges/ data.num_nodes: .2f}')\n",
        "print(f'has self_loops: {data.has_self_loops()}')\n",
        "print(f'has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'is directional: {data.is_directed()}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ne6KdJUMlMb",
        "outputId": "9cc57338-a0b4-41a2-9847-2d5688cf07a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset: PubMed()\n",
            "num of graphes: 1\n",
            "num of node features:500\n",
            "num of classes:3\n",
            "data: Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
            "Num of Nodes:19717\n",
            "Num of edges:88648\n",
            "Average node degree:  4.50\n",
            "has self_loops: False\n",
            "has isolated nodes: False\n",
            "is directional: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use Cluster Data and ClusterLoader to cluster nodes\n",
        "#source code for ClusterData https://pytorch-geometric.readthedocs.io/en/1.4.3/_modules/torch_geometric/data/cluster.html\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "# The first step: cluster data\n",
        "cluster_data = ClusterData(data, num_parts=128) # create 128 subgraphes\n",
        "# concat clustered nodes into a larger graph in a stochastic fashion\n",
        "train_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True) # make 4 batches\n",
        "\n",
        "next = next(iter(train_loader))\n",
        "\n",
        "print(next)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP6eVdo0OqHe",
        "outputId": "53fdd624-56e3-4818-c34a-0ad662ffb7aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing METIS partitioning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[4909, 500], y=[4909], train_mask=[4909], val_mask=[4909], test_mask=[4909], edge_index=[2, 15304])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next.test_mask == next.val_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUPUlYzUZqWV",
        "outputId": "b360a1fe-befb-46fe-8ded-098c54908743"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True,  ..., True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build model_0\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch.nn import Linear\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    self.conv1=GCNConv(in_channels=dataset.num_features, out_channels=hidden_channels)\n",
        "    self.conv2 =GCNConv(in_channels= hidden_channels, out_channels=hidden_channels)\n",
        "    self.lin1 = Linear(in_features = hidden_channels, out_features= dataset.num_classes)\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1 (x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = self.lin1(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model_0 = GCN(hidden_channels=16)\n",
        "model_0\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPdpONUUTlkP",
        "outputId": "721ab03a-9f81-45ce-b8ea-c5ea05946e72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(500, 16)\n",
              "  (conv2): GCNConv(16, 16)\n",
              "  (lin1): Linear(in_features=16, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "import torch_geometric\n",
        "\n",
        "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model:torch.nn.Module, dataloader: torch_geometric.data.DataLoader, loss_fn:torch.nn.Module, optimizer: torch.optim.Optimizer):\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "  for subdata in dataloader:\n",
        "    out = model(subdata.x, subdata.edge_index)\n",
        "    loss = loss_fn(out[subdata.train_mask], subdata.y[subdata.train_mask])\n",
        "    train_loss +=loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# for test, no need to partion the graph\n",
        "\n",
        "def test(model:torch.nn.Module, data: torch_geometric.data.Data, loss_fn:torch.nn.Module ):\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    out = model(data.x, data.edge_index)\n",
        "    pred = out.argmax(dim=1)\n",
        "    accs =[]\n",
        "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
        "      correct = (pred[mask]==data.y[mask]).sum().item()\n",
        "      accs.append(correct/int(mask.sum()))\n",
        "\n",
        "  return accs\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "  loss = train(model_0, train_loader, loss_fn=loss_fn, optimizer=optimizer)\n",
        "  train_acc, val_acc, test_acc = test(model_0, data=data, loss_fn=loss_fn)\n",
        "  print(f'Epoch: {epoch} train: {train_acc: 3f} val :{val_acc:.3f} test: {test_acc:3f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "dMiJCBj7W_ag",
        "outputId": "b9025d46-1088-4ec6-f958-00eb7bc2fb4d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 train:  0.333333 val :0.196 test: 0.180000\n",
            "Epoch: 2 train:  0.533333 val :0.414 test: 0.397000\n",
            "Epoch: 3 train:  0.650000 val :0.536 test: 0.527000\n",
            "Epoch: 4 train:  0.700000 val :0.554 test: 0.556000\n",
            "Epoch: 5 train:  0.716667 val :0.576 test: 0.578000\n",
            "Epoch: 6 train:  0.866667 val :0.700 test: 0.690000\n",
            "Epoch: 7 train:  0.916667 val :0.754 test: 0.738000\n",
            "Epoch: 8 train:  0.900000 val :0.704 test: 0.691000\n",
            "Epoch: 9 train:  0.950000 val :0.744 test: 0.729000\n",
            "Epoch: 10 train:  0.850000 val :0.634 test: 0.610000\n",
            "Epoch: 11 train:  0.983333 val :0.708 test: 0.683000\n",
            "Epoch: 12 train:  0.950000 val :0.742 test: 0.719000\n",
            "Epoch: 13 train:  0.983333 val :0.762 test: 0.742000\n",
            "Epoch: 14 train:  1.000000 val :0.782 test: 0.736000\n",
            "Epoch: 15 train:  1.000000 val :0.778 test: 0.747000\n",
            "Epoch: 16 train:  0.983333 val :0.762 test: 0.751000\n",
            "Epoch: 17 train:  1.000000 val :0.780 test: 0.732000\n",
            "Epoch: 18 train:  1.000000 val :0.778 test: 0.722000\n",
            "Epoch: 19 train:  0.983333 val :0.764 test: 0.745000\n",
            "Epoch: 20 train:  1.000000 val :0.786 test: 0.760000\n",
            "Epoch: 21 train:  1.000000 val :0.792 test: 0.757000\n",
            "Epoch: 22 train:  1.000000 val :0.786 test: 0.763000\n",
            "Epoch: 23 train:  1.000000 val :0.772 test: 0.758000\n",
            "Epoch: 24 train:  1.000000 val :0.772 test: 0.754000\n",
            "Epoch: 25 train:  1.000000 val :0.786 test: 0.762000\n",
            "Epoch: 26 train:  0.983333 val :0.798 test: 0.764000\n",
            "Epoch: 27 train:  0.983333 val :0.790 test: 0.765000\n",
            "Epoch: 28 train:  1.000000 val :0.766 test: 0.749000\n",
            "Epoch: 29 train:  1.000000 val :0.766 test: 0.747000\n",
            "Epoch: 30 train:  1.000000 val :0.788 test: 0.771000\n",
            "Epoch: 31 train:  1.000000 val :0.794 test: 0.767000\n",
            "Epoch: 32 train:  1.000000 val :0.802 test: 0.770000\n",
            "Epoch: 33 train:  1.000000 val :0.788 test: 0.769000\n",
            "Epoch: 34 train:  1.000000 val :0.770 test: 0.754000\n",
            "Epoch: 35 train:  1.000000 val :0.786 test: 0.775000\n",
            "Epoch: 36 train:  1.000000 val :0.800 test: 0.776000\n",
            "Epoch: 37 train:  1.000000 val :0.794 test: 0.778000\n",
            "Epoch: 38 train:  1.000000 val :0.800 test: 0.777000\n",
            "Epoch: 39 train:  1.000000 val :0.780 test: 0.771000\n",
            "Epoch: 40 train:  1.000000 val :0.776 test: 0.760000\n",
            "Epoch: 41 train:  1.000000 val :0.776 test: 0.754000\n",
            "Epoch: 42 train:  1.000000 val :0.768 test: 0.761000\n",
            "Epoch: 43 train:  1.000000 val :0.784 test: 0.772000\n",
            "Epoch: 44 train:  0.983333 val :0.796 test: 0.775000\n",
            "Epoch: 45 train:  0.983333 val :0.804 test: 0.782000\n",
            "Epoch: 46 train:  1.000000 val :0.772 test: 0.760000\n",
            "Epoch: 47 train:  1.000000 val :0.766 test: 0.757000\n",
            "Epoch: 48 train:  1.000000 val :0.788 test: 0.771000\n",
            "Epoch: 49 train:  0.983333 val :0.798 test: 0.782000\n",
            "Epoch: 50 train:  0.983333 val :0.810 test: 0.783000\n",
            "Epoch: 51 train:  1.000000 val :0.816 test: 0.783000\n",
            "Epoch: 52 train:  1.000000 val :0.792 test: 0.777000\n",
            "Epoch: 53 train:  1.000000 val :0.784 test: 0.764000\n",
            "Epoch: 54 train:  1.000000 val :0.802 test: 0.772000\n",
            "Epoch: 55 train:  1.000000 val :0.802 test: 0.772000\n",
            "Epoch: 56 train:  1.000000 val :0.788 test: 0.776000\n",
            "Epoch: 57 train:  1.000000 val :0.772 test: 0.751000\n",
            "Epoch: 58 train:  1.000000 val :0.774 test: 0.757000\n",
            "Epoch: 59 train:  1.000000 val :0.798 test: 0.776000\n",
            "Epoch: 60 train:  1.000000 val :0.800 test: 0.779000\n",
            "Epoch: 61 train:  1.000000 val :0.800 test: 0.779000\n",
            "Epoch: 62 train:  1.000000 val :0.802 test: 0.780000\n",
            "Epoch: 63 train:  1.000000 val :0.796 test: 0.776000\n",
            "Epoch: 64 train:  1.000000 val :0.804 test: 0.779000\n",
            "Epoch: 65 train:  1.000000 val :0.802 test: 0.781000\n",
            "Epoch: 66 train:  1.000000 val :0.804 test: 0.779000\n",
            "Epoch: 67 train:  1.000000 val :0.798 test: 0.778000\n",
            "Epoch: 68 train:  1.000000 val :0.778 test: 0.770000\n",
            "Epoch: 69 train:  1.000000 val :0.796 test: 0.779000\n",
            "Epoch: 70 train:  1.000000 val :0.796 test: 0.779000\n",
            "Epoch: 71 train:  1.000000 val :0.788 test: 0.770000\n",
            "Epoch: 72 train:  1.000000 val :0.774 test: 0.768000\n",
            "Epoch: 73 train:  1.000000 val :0.796 test: 0.781000\n",
            "Epoch: 74 train:  1.000000 val :0.774 test: 0.759000\n",
            "Epoch: 75 train:  1.000000 val :0.784 test: 0.763000\n",
            "Epoch: 76 train:  1.000000 val :0.760 test: 0.746000\n",
            "Epoch: 77 train:  1.000000 val :0.754 test: 0.730000\n",
            "Epoch: 78 train:  1.000000 val :0.800 test: 0.777000\n",
            "Epoch: 79 train:  1.000000 val :0.774 test: 0.765000\n",
            "Epoch: 80 train:  1.000000 val :0.770 test: 0.757000\n",
            "Epoch: 81 train:  1.000000 val :0.788 test: 0.769000\n",
            "Epoch: 82 train:  1.000000 val :0.798 test: 0.774000\n",
            "Epoch: 83 train:  1.000000 val :0.792 test: 0.773000\n",
            "Epoch: 84 train:  1.000000 val :0.778 test: 0.764000\n",
            "Epoch: 85 train:  1.000000 val :0.794 test: 0.777000\n",
            "Epoch: 86 train:  1.000000 val :0.784 test: 0.778000\n",
            "Epoch: 87 train:  1.000000 val :0.794 test: 0.775000\n",
            "Epoch: 88 train:  1.000000 val :0.788 test: 0.772000\n",
            "Epoch: 89 train:  1.000000 val :0.774 test: 0.766000\n",
            "Epoch: 90 train:  1.000000 val :0.778 test: 0.770000\n",
            "Epoch: 91 train:  1.000000 val :0.784 test: 0.775000\n",
            "Epoch: 92 train:  1.000000 val :0.792 test: 0.778000\n",
            "Epoch: 93 train:  0.983333 val :0.804 test: 0.788000\n",
            "Epoch: 94 train:  1.000000 val :0.798 test: 0.778000\n",
            "Epoch: 95 train:  1.000000 val :0.788 test: 0.771000\n",
            "Epoch: 96 train:  1.000000 val :0.792 test: 0.775000\n",
            "Epoch: 97 train:  1.000000 val :0.794 test: 0.782000\n",
            "Epoch: 98 train:  1.000000 val :0.794 test: 0.779000\n",
            "Epoch: 99 train:  1.000000 val :0.758 test: 0.755000\n",
            "Epoch: 100 train:  1.000000 val :0.778 test: 0.764000\n"
          ]
        }
      ]
    }
  ]
}