{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIWGE04VE1tAw67/nM24il",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadwin-357/GCN/blob/main/Graph_level_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CruyKHbBl_8P",
        "outputId": "16b303b7-7f3b-4a12-e488-78c527436371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Graph classification: need to distinguish differnt graph:\n",
        "# use molecular property prediction as a example\n",
        "#data from torch_geometric.datasets.TUDataset (from TU Dortmund University)\n",
        "\n",
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "#GET familar with the data\n",
        "print (f'Dataset:{dataset}')\n",
        "print(f'Num of graphes:{len(dataset)}')\n",
        "print(f'Num of features:{dataset.num_features}')\n",
        "print(f'Num of classes:{dataset.num_classes}')\n",
        "\n",
        "#get the first data\n",
        "data= dataset[0]\n",
        "\n",
        "print(f'First data: {data}')\n",
        "print(f'Num of Nodes:{data.num_nodes}')\n",
        "print(f'Num of Edges:{data.num_edges}')\n",
        "print(f'Average Node degrees:{data.num_edges/data.num_nodes: 2f}')\n",
        "print(f'Has isolated nodes:{data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops:{data.has_self_loops()}')\n",
        "print(f'is directional:{data.is_directed()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-2tGdimE-k",
        "outputId": "18e9245e-9c0a-4ab5-99fc-4053a92fefbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:MUTAG(188)\n",
            "Num of graphes:188\n",
            "Num of features:7\n",
            "Num of classes:2\n",
            "First data: Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "Num of Nodes:17\n",
            "Num of Edges:38\n",
            "Average Node degrees: 2.235294\n",
            "Has isolated nodes:False\n",
            "Has self-loops:False\n",
            "is directional:False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle the data and split the data into train and test\n",
        "torch.manual_seed(42)\n",
        "dataset  = dataset.shuffle()\n",
        "\n",
        "train_dataset =dataset[:150]\n",
        "test_dataset =dataset[150:]\n",
        "\n",
        "print(f'num of training graphs:{train_dataset} num of testing graphs:{test_dataset}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-24CgO9ohYg",
        "outputId": "bf30c35a-7bd5-4a41-8ef7-a83a5ff62262"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of training graphs:MUTAG(150) num of testing graphs:MUTAG(38)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batching in graph learning\n",
        "from torch_geometric.loader import DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle =True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "for i , data in enumerate(train_dataloader):\n",
        "  print(f'Bath :{i+1}')\n",
        "  print(f'Num of graphes in this batch:{data.num_graphs}')\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl0ShJbEvQvT",
        "outputId": "de9b6209-9414-4de7-9e08-d990a31a75af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bath :1\n",
            "Num of graphes in this batch:32\n",
            "DataBatch(edge_index=[2, 1202], x=[550, 7], edge_attr=[1202, 4], y=[32], batch=[550], ptr=[33])\n",
            "Bath :2\n",
            "Num of graphes in this batch:32\n",
            "DataBatch(edge_index=[2, 1316], x=[589, 7], edge_attr=[1316, 4], y=[32], batch=[589], ptr=[33])\n",
            "Bath :3\n",
            "Num of graphes in this batch:32\n",
            "DataBatch(edge_index=[2, 1266], x=[569, 7], edge_attr=[1266, 4], y=[32], batch=[569], ptr=[33])\n",
            "Bath :4\n",
            "Num of graphes in this batch:32\n",
            "DataBatch(edge_index=[2, 1252], x=[574, 7], edge_attr=[1252, 4], y=[32], batch=[574], ptr=[33])\n",
            "Bath :5\n",
            "Num of graphes in this batch:22\n",
            "DataBatch(edge_index=[2, 940], x=[421, 7], edge_attr=[940, 4], y=[22], batch=[421], ptr=[23])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "wAx0-l-VPsUR",
        "outputId": "d3c230a7-203f-40a5-8810-e3f3d68a4b40"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TUDataset' object has no attribute 'num_graphs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-61fe8cb63db4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_data_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise AttributeError(f\"'{self.__class__.__name__}' object has no \"\n\u001b[0m\u001b[1;32m    320\u001b[0m                              f\"attribute '{key}'\")\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TUDataset' object has no attribute 'num_graphs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn.pool import global_add_pool\n",
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# model_0, edge features are not used\n",
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    torch.manual_seed(32)\n",
        "    self.conv1 = GCNConv(in_channels=dataset.num_features, out_channels=hidden_channels)\n",
        "    self.conv2 = GCNConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
        "    self.conv3 = GCNConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
        "    self.lin = Linear(in_features=hidden_channels, out_features=dataset.num_classes)\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = x.relu()\n",
        "    x = self.conv3(x, edge_index)\n",
        "\n",
        "    # readout layer using Mean aggregation\n",
        "    x = global_add_pool(x, batch) # batch info is needed to calculate the size of each sample in the batch)\n",
        "\n",
        "    # final classifier layer\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.lin(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "model_0 = GCN(hidden_channels=64)\n",
        "print (model_0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCNH6wgkxPpU",
        "outputId": "da888200-3b30-494f-d80f-4650080c70d6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "import torch_geometric\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          dataloader: torch_geometric.loader.DataLoader,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          optimizer: torch.optim.Optimizer):\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "  for data in dataloader:\n",
        "    out = model(data.x, data.edge_index, data.batch)\n",
        "    loss =loss_fn(out, data.y)\n",
        "    train_loss +=loss\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_acc +=(out.argmax(dim=1)==data.y).sum()/len(data.y)\n",
        "\n",
        "  #normalize the train_loss and tran_acc over batch\n",
        "  train_loss = train_loss/len(dataloader)\n",
        "  train_acc = train_acc/len(dataloader)\n",
        "\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test(model: torch.nn.Module,\n",
        "          dataloader: torch_geometric.loader.DataLoader,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          ):\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for data in dataloader:\n",
        "      out = model(data.x, data.edge_index, data.batch)\n",
        "      loss =loss_fn(out, data.y)\n",
        "      test_loss +=loss\n",
        "      test_acc += (out.argmax(dim=1)==data.y).sum()/len(data.y)\n",
        "\n",
        "  #normalize the train_loss and tran_acc over batch\n",
        "  test_loss = test_loss/len(dataloader)\n",
        "  test_acc = test_acc/len(dataloader)\n",
        "\n",
        "  return test_loss, test_acc\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_0.parameters(), lr =0.01, weight_decay=1e-4)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "  train_loss, train_acc=train(model_0, train_dataloader, loss_fn=loss_fn, optimizer=optimizer)\n",
        "  test_loss, test_acc = test(model_0, test_dataloader, loss_fn= loss_fn)\n",
        "  print(f'Epoch: {epoch}: train_loss/test_loss: {train_loss:2f}/{test_loss:2f} train_acc/test_acc:{train_acc:2f}/{test_acc:2f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Zhhg0pWUHSgv",
        "outputId": "752b11b9-a3a9-475b-82b0-4a73a0407607"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1: train_loss/test_loss: 0.432701/0.708158 train_acc/test_acc:0.779545/0.692708\n",
            "Epoch: 2: train_loss/test_loss: 0.411723/0.669521 train_acc/test_acc:0.837500/0.677083\n",
            "Epoch: 3: train_loss/test_loss: 0.427356/0.706128 train_acc/test_acc:0.838636/0.739583\n",
            "Epoch: 4: train_loss/test_loss: 0.413721/0.731951 train_acc/test_acc:0.803977/0.661458\n",
            "Epoch: 5: train_loss/test_loss: 0.342158/0.655460 train_acc/test_acc:0.860227/0.723958\n",
            "Epoch: 6: train_loss/test_loss: 0.426444/0.678379 train_acc/test_acc:0.786364/0.692708\n",
            "Epoch: 7: train_loss/test_loss: 0.368577/0.700707 train_acc/test_acc:0.828977/0.723958\n",
            "Epoch: 8: train_loss/test_loss: 0.400333/0.743533 train_acc/test_acc:0.838636/0.677083\n",
            "Epoch: 9: train_loss/test_loss: 0.359094/0.698822 train_acc/test_acc:0.825568/0.692708\n",
            "Epoch: 10: train_loss/test_loss: 0.357790/0.776389 train_acc/test_acc:0.869318/0.645833\n",
            "Epoch: 11: train_loss/test_loss: 0.347746/0.794220 train_acc/test_acc:0.847727/0.708333\n",
            "Epoch: 12: train_loss/test_loss: 0.377924/0.716075 train_acc/test_acc:0.831818/0.692708\n",
            "Epoch: 13: train_loss/test_loss: 0.339474/0.785763 train_acc/test_acc:0.869886/0.692708\n",
            "Epoch: 14: train_loss/test_loss: 0.390002/0.815823 train_acc/test_acc:0.798864/0.692708\n",
            "Epoch: 15: train_loss/test_loss: 0.386292/0.766568 train_acc/test_acc:0.823295/0.692708\n",
            "Epoch: 16: train_loss/test_loss: 0.354210/0.785759 train_acc/test_acc:0.848864/0.692708\n",
            "Epoch: 17: train_loss/test_loss: 0.336084/0.823565 train_acc/test_acc:0.814205/0.677083\n",
            "Epoch: 18: train_loss/test_loss: 0.362452/0.829275 train_acc/test_acc:0.844318/0.677083\n",
            "Epoch: 19: train_loss/test_loss: 0.386596/0.758388 train_acc/test_acc:0.834659/0.723958\n",
            "Epoch: 20: train_loss/test_loss: 0.434383/0.785222 train_acc/test_acc:0.788636/0.661458\n",
            "Epoch: 21: train_loss/test_loss: 0.389505/0.818757 train_acc/test_acc:0.842045/0.723958\n",
            "Epoch: 22: train_loss/test_loss: 0.393021/0.790332 train_acc/test_acc:0.805114/0.692708\n",
            "Epoch: 23: train_loss/test_loss: 0.384108/0.839918 train_acc/test_acc:0.842045/0.739583\n",
            "Epoch: 24: train_loss/test_loss: 0.352879/0.749100 train_acc/test_acc:0.831818/0.692708\n",
            "Epoch: 25: train_loss/test_loss: 0.349519/0.844299 train_acc/test_acc:0.853977/0.708333\n",
            "Epoch: 26: train_loss/test_loss: 0.347762/0.914593 train_acc/test_acc:0.830114/0.708333\n",
            "Epoch: 27: train_loss/test_loss: 0.378201/0.772276 train_acc/test_acc:0.798295/0.723958\n",
            "Epoch: 28: train_loss/test_loss: 0.354485/0.764515 train_acc/test_acc:0.834659/0.677083\n",
            "Epoch: 29: train_loss/test_loss: 0.382747/0.890811 train_acc/test_acc:0.828977/0.723958\n",
            "Epoch: 30: train_loss/test_loss: 0.393522/0.876882 train_acc/test_acc:0.807955/0.677083\n",
            "Epoch: 31: train_loss/test_loss: 0.360015/0.763637 train_acc/test_acc:0.819318/0.739583\n",
            "Epoch: 32: train_loss/test_loss: 0.383491/0.782840 train_acc/test_acc:0.838636/0.677083\n",
            "Epoch: 33: train_loss/test_loss: 0.346317/0.895100 train_acc/test_acc:0.819886/0.723958\n",
            "Epoch: 34: train_loss/test_loss: 0.382976/0.808537 train_acc/test_acc:0.810795/0.708333\n",
            "Epoch: 35: train_loss/test_loss: 0.357089/0.779516 train_acc/test_acc:0.822727/0.677083\n",
            "Epoch: 36: train_loss/test_loss: 0.343365/0.788661 train_acc/test_acc:0.834659/0.692708\n",
            "Epoch: 37: train_loss/test_loss: 0.359158/0.820263 train_acc/test_acc:0.835795/0.677083\n",
            "Epoch: 38: train_loss/test_loss: 0.370487/0.830943 train_acc/test_acc:0.832386/0.692708\n",
            "Epoch: 39: train_loss/test_loss: 0.356809/0.834190 train_acc/test_acc:0.822727/0.677083\n",
            "Epoch: 40: train_loss/test_loss: 0.404388/0.855358 train_acc/test_acc:0.810795/0.723958\n",
            "Epoch: 41: train_loss/test_loss: 0.398505/0.757605 train_acc/test_acc:0.831818/0.661458\n",
            "Epoch: 42: train_loss/test_loss: 0.380483/0.852955 train_acc/test_acc:0.840909/0.739583\n",
            "Epoch: 43: train_loss/test_loss: 0.360426/0.831362 train_acc/test_acc:0.832386/0.692708\n",
            "Epoch: 44: train_loss/test_loss: 0.355918/0.823570 train_acc/test_acc:0.826705/0.708333\n",
            "Epoch: 45: train_loss/test_loss: 0.380456/0.770624 train_acc/test_acc:0.825000/0.708333\n",
            "Epoch: 46: train_loss/test_loss: 0.373411/0.887655 train_acc/test_acc:0.832386/0.708333\n",
            "Epoch: 47: train_loss/test_loss: 0.394532/0.820683 train_acc/test_acc:0.814773/0.708333\n",
            "Epoch: 48: train_loss/test_loss: 0.356437/0.706737 train_acc/test_acc:0.829545/0.692708\n",
            "Epoch: 49: train_loss/test_loss: 0.345005/0.826892 train_acc/test_acc:0.878409/0.708333\n",
            "Epoch: 50: train_loss/test_loss: 0.315298/0.885480 train_acc/test_acc:0.838068/0.677083\n",
            "Epoch: 51: train_loss/test_loss: 0.318035/0.856858 train_acc/test_acc:0.844318/0.692708\n",
            "Epoch: 52: train_loss/test_loss: 0.372202/0.869513 train_acc/test_acc:0.876136/0.708333\n",
            "Epoch: 53: train_loss/test_loss: 0.357072/0.857508 train_acc/test_acc:0.838636/0.692708\n",
            "Epoch: 54: train_loss/test_loss: 0.375220/0.907256 train_acc/test_acc:0.822159/0.739583\n",
            "Epoch: 55: train_loss/test_loss: 0.332705/0.871193 train_acc/test_acc:0.878977/0.677083\n",
            "Epoch: 56: train_loss/test_loss: 0.396923/0.740136 train_acc/test_acc:0.829545/0.692708\n",
            "Epoch: 57: train_loss/test_loss: 0.392462/0.795587 train_acc/test_acc:0.826136/0.739583\n",
            "Epoch: 58: train_loss/test_loss: 0.361638/0.859595 train_acc/test_acc:0.835227/0.692708\n",
            "Epoch: 59: train_loss/test_loss: 0.339903/0.801897 train_acc/test_acc:0.848295/0.723958\n",
            "Epoch: 60: train_loss/test_loss: 0.387489/0.774971 train_acc/test_acc:0.826136/0.708333\n",
            "Epoch: 61: train_loss/test_loss: 0.344808/0.856463 train_acc/test_acc:0.831818/0.692708\n",
            "Epoch: 62: train_loss/test_loss: 0.359762/0.906566 train_acc/test_acc:0.835227/0.723958\n",
            "Epoch: 63: train_loss/test_loss: 0.336421/0.834431 train_acc/test_acc:0.850000/0.692708\n",
            "Epoch: 64: train_loss/test_loss: 0.388414/0.901453 train_acc/test_acc:0.801705/0.708333\n",
            "Epoch: 65: train_loss/test_loss: 0.340350/0.911907 train_acc/test_acc:0.863636/0.708333\n",
            "Epoch: 66: train_loss/test_loss: 0.364303/0.847478 train_acc/test_acc:0.807386/0.723958\n",
            "Epoch: 67: train_loss/test_loss: 0.372867/0.830473 train_acc/test_acc:0.848295/0.677083\n",
            "Epoch: 68: train_loss/test_loss: 0.408724/0.855624 train_acc/test_acc:0.844318/0.661458\n",
            "Epoch: 69: train_loss/test_loss: 0.364691/0.816619 train_acc/test_acc:0.826136/0.739583\n",
            "Epoch: 70: train_loss/test_loss: 0.372100/0.811935 train_acc/test_acc:0.857386/0.692708\n",
            "Epoch: 71: train_loss/test_loss: 0.407626/0.738394 train_acc/test_acc:0.804545/0.677083\n",
            "Epoch: 72: train_loss/test_loss: 0.399805/0.783913 train_acc/test_acc:0.853977/0.692708\n",
            "Epoch: 73: train_loss/test_loss: 0.390493/0.826592 train_acc/test_acc:0.823864/0.708333\n",
            "Epoch: 74: train_loss/test_loss: 0.354853/0.807794 train_acc/test_acc:0.860227/0.723958\n",
            "Epoch: 75: train_loss/test_loss: 0.372991/0.751539 train_acc/test_acc:0.804545/0.677083\n",
            "Epoch: 76: train_loss/test_loss: 0.362956/0.882929 train_acc/test_acc:0.857955/0.677083\n",
            "Epoch: 77: train_loss/test_loss: 0.335607/0.775665 train_acc/test_acc:0.835795/0.661458\n",
            "Epoch: 78: train_loss/test_loss: 0.369215/0.751608 train_acc/test_acc:0.835227/0.692708\n",
            "Epoch: 79: train_loss/test_loss: 0.358530/0.879563 train_acc/test_acc:0.819318/0.692708\n",
            "Epoch: 80: train_loss/test_loss: 0.329731/0.893322 train_acc/test_acc:0.841477/0.708333\n",
            "Epoch: 81: train_loss/test_loss: 0.328131/0.793188 train_acc/test_acc:0.844318/0.645833\n",
            "Epoch: 82: train_loss/test_loss: 0.387388/0.895031 train_acc/test_acc:0.835795/0.708333\n",
            "Epoch: 83: train_loss/test_loss: 0.334423/0.877139 train_acc/test_acc:0.853409/0.677083\n",
            "Epoch: 84: train_loss/test_loss: 0.372362/0.859520 train_acc/test_acc:0.832955/0.692708\n",
            "Epoch: 85: train_loss/test_loss: 0.383092/0.787287 train_acc/test_acc:0.821023/0.692708\n",
            "Epoch: 86: train_loss/test_loss: 0.335193/0.825983 train_acc/test_acc:0.835227/0.692708\n",
            "Epoch: 87: train_loss/test_loss: 0.322975/0.867507 train_acc/test_acc:0.867045/0.723958\n",
            "Epoch: 88: train_loss/test_loss: 0.377833/0.720939 train_acc/test_acc:0.819886/0.692708\n",
            "Epoch: 89: train_loss/test_loss: 0.379170/0.833767 train_acc/test_acc:0.832386/0.677083\n",
            "Epoch: 90: train_loss/test_loss: 0.326219/0.904597 train_acc/test_acc:0.823295/0.671875\n",
            "Epoch: 91: train_loss/test_loss: 0.358522/0.812643 train_acc/test_acc:0.822159/0.661458\n",
            "Epoch: 92: train_loss/test_loss: 0.380039/0.856755 train_acc/test_acc:0.794886/0.708333\n",
            "Epoch: 93: train_loss/test_loss: 0.382817/0.753571 train_acc/test_acc:0.832386/0.692708\n",
            "Epoch: 94: train_loss/test_loss: 0.305864/0.782124 train_acc/test_acc:0.856818/0.739583\n",
            "Epoch: 95: train_loss/test_loss: 0.404271/0.928161 train_acc/test_acc:0.798864/0.677083\n",
            "Epoch: 96: train_loss/test_loss: 0.408345/0.804038 train_acc/test_acc:0.844886/0.723958\n",
            "Epoch: 97: train_loss/test_loss: 0.366481/0.768755 train_acc/test_acc:0.851136/0.677083\n",
            "Epoch: 98: train_loss/test_loss: 0.425317/0.900415 train_acc/test_acc:0.823295/0.723958\n",
            "Epoch: 99: train_loss/test_loss: 0.388843/0.976905 train_acc/test_acc:0.847727/0.677083\n",
            "Epoch: 100: train_loss/test_loss: 0.353944/0.880476 train_acc/test_acc:0.826136/0.739583\n",
            "Epoch: 101: train_loss/test_loss: 0.334009/0.806584 train_acc/test_acc:0.860227/0.645833\n",
            "Epoch: 102: train_loss/test_loss: 0.350351/0.928907 train_acc/test_acc:0.844318/0.755208\n",
            "Epoch: 103: train_loss/test_loss: 0.353122/0.815562 train_acc/test_acc:0.851136/0.661458\n",
            "Epoch: 104: train_loss/test_loss: 0.399654/0.727488 train_acc/test_acc:0.832386/0.723958\n",
            "Epoch: 105: train_loss/test_loss: 0.363505/0.817199 train_acc/test_acc:0.825568/0.692708\n",
            "Epoch: 106: train_loss/test_loss: 0.346641/0.852662 train_acc/test_acc:0.841477/0.677083\n",
            "Epoch: 107: train_loss/test_loss: 0.317215/0.839310 train_acc/test_acc:0.863068/0.708333\n",
            "Epoch: 108: train_loss/test_loss: 0.288641/0.780819 train_acc/test_acc:0.875568/0.708333\n",
            "Epoch: 109: train_loss/test_loss: 0.354474/0.805722 train_acc/test_acc:0.863636/0.692708\n",
            "Epoch: 110: train_loss/test_loss: 0.351068/0.933663 train_acc/test_acc:0.856818/0.708333\n",
            "Epoch: 111: train_loss/test_loss: 0.336141/0.844316 train_acc/test_acc:0.851705/0.708333\n",
            "Epoch: 112: train_loss/test_loss: 0.351664/0.812258 train_acc/test_acc:0.857386/0.692708\n",
            "Epoch: 113: train_loss/test_loss: 0.305911/0.856116 train_acc/test_acc:0.856818/0.692708\n",
            "Epoch: 114: train_loss/test_loss: 0.396660/1.032701 train_acc/test_acc:0.854545/0.708333\n",
            "Epoch: 115: train_loss/test_loss: 0.395182/0.794047 train_acc/test_acc:0.826136/0.692708\n",
            "Epoch: 116: train_loss/test_loss: 0.359508/0.800169 train_acc/test_acc:0.829545/0.692708\n",
            "Epoch: 117: train_loss/test_loss: 0.330398/0.860708 train_acc/test_acc:0.854545/0.723958\n",
            "Epoch: 118: train_loss/test_loss: 0.332198/0.794769 train_acc/test_acc:0.835795/0.692708\n",
            "Epoch: 119: train_loss/test_loss: 0.340370/0.800459 train_acc/test_acc:0.856818/0.723958\n",
            "Epoch: 120: train_loss/test_loss: 0.320024/0.760751 train_acc/test_acc:0.853409/0.708333\n",
            "Epoch: 121: train_loss/test_loss: 0.345945/0.876635 train_acc/test_acc:0.853977/0.708333\n",
            "Epoch: 122: train_loss/test_loss: 0.346567/0.783000 train_acc/test_acc:0.857386/0.723958\n",
            "Epoch: 123: train_loss/test_loss: 0.382603/0.711429 train_acc/test_acc:0.828977/0.692708\n",
            "Epoch: 124: train_loss/test_loss: 0.393985/0.891666 train_acc/test_acc:0.805682/0.739583\n",
            "Epoch: 125: train_loss/test_loss: 0.379203/0.916205 train_acc/test_acc:0.854545/0.692708\n",
            "Epoch: 126: train_loss/test_loss: 0.377482/0.801748 train_acc/test_acc:0.844886/0.739583\n",
            "Epoch: 127: train_loss/test_loss: 0.320086/0.799488 train_acc/test_acc:0.869318/0.677083\n",
            "Epoch: 128: train_loss/test_loss: 0.335529/0.897471 train_acc/test_acc:0.826136/0.723958\n",
            "Epoch: 129: train_loss/test_loss: 0.301257/0.953547 train_acc/test_acc:0.863636/0.677083\n",
            "Epoch: 130: train_loss/test_loss: 0.346664/0.878770 train_acc/test_acc:0.860227/0.723958\n",
            "Epoch: 131: train_loss/test_loss: 0.288871/0.781307 train_acc/test_acc:0.869886/0.739583\n",
            "Epoch: 132: train_loss/test_loss: 0.386889/0.777176 train_acc/test_acc:0.839205/0.739583\n",
            "Epoch: 133: train_loss/test_loss: 0.297393/0.925837 train_acc/test_acc:0.869318/0.692708\n",
            "Epoch: 134: train_loss/test_loss: 0.329147/0.939466 train_acc/test_acc:0.856818/0.708333\n",
            "Epoch: 135: train_loss/test_loss: 0.410388/1.005267 train_acc/test_acc:0.807386/0.770833\n",
            "Epoch: 136: train_loss/test_loss: 0.405924/0.856296 train_acc/test_acc:0.813068/0.692708\n",
            "Epoch: 137: train_loss/test_loss: 0.403338/0.775325 train_acc/test_acc:0.832386/0.692708\n",
            "Epoch: 138: train_loss/test_loss: 0.317435/0.799623 train_acc/test_acc:0.881818/0.739583\n",
            "Epoch: 139: train_loss/test_loss: 0.331611/0.874928 train_acc/test_acc:0.847727/0.661458\n",
            "Epoch: 140: train_loss/test_loss: 0.381721/0.875576 train_acc/test_acc:0.795455/0.671875\n",
            "Epoch: 141: train_loss/test_loss: 0.372071/0.812053 train_acc/test_acc:0.826136/0.739583\n",
            "Epoch: 142: train_loss/test_loss: 0.338102/0.780270 train_acc/test_acc:0.817045/0.578125\n",
            "Epoch: 143: train_loss/test_loss: 0.333789/0.829209 train_acc/test_acc:0.832386/0.739583\n",
            "Epoch: 144: train_loss/test_loss: 0.371543/0.927701 train_acc/test_acc:0.865909/0.677083\n",
            "Epoch: 145: train_loss/test_loss: 0.335191/0.890755 train_acc/test_acc:0.835227/0.723958\n",
            "Epoch: 146: train_loss/test_loss: 0.323496/0.741458 train_acc/test_acc:0.863636/0.708333\n",
            "Epoch: 147: train_loss/test_loss: 0.268771/0.886443 train_acc/test_acc:0.860227/0.692708\n",
            "Epoch: 148: train_loss/test_loss: 0.354915/1.026149 train_acc/test_acc:0.816477/0.723958\n",
            "Epoch: 149: train_loss/test_loss: 0.362453/0.917804 train_acc/test_acc:0.869318/0.677083\n",
            "Epoch: 150: train_loss/test_loss: 0.379821/0.837227 train_acc/test_acc:0.804545/0.739583\n",
            "Epoch: 151: train_loss/test_loss: 0.380393/0.959145 train_acc/test_acc:0.817045/0.645833\n",
            "Epoch: 152: train_loss/test_loss: 0.344037/0.968102 train_acc/test_acc:0.853977/0.739583\n",
            "Epoch: 153: train_loss/test_loss: 0.324502/0.926021 train_acc/test_acc:0.836364/0.692708\n",
            "Epoch: 154: train_loss/test_loss: 0.326097/0.853669 train_acc/test_acc:0.866477/0.692708\n",
            "Epoch: 155: train_loss/test_loss: 0.375870/0.978662 train_acc/test_acc:0.847727/0.739583\n",
            "Epoch: 156: train_loss/test_loss: 0.359725/0.883025 train_acc/test_acc:0.841477/0.677083\n",
            "Epoch: 157: train_loss/test_loss: 0.362717/0.869636 train_acc/test_acc:0.856250/0.739583\n",
            "Epoch: 158: train_loss/test_loss: 0.388326/0.916666 train_acc/test_acc:0.830682/0.692708\n",
            "Epoch: 159: train_loss/test_loss: 0.370712/0.737948 train_acc/test_acc:0.866477/0.708333\n",
            "Epoch: 160: train_loss/test_loss: 0.317619/0.849576 train_acc/test_acc:0.863636/0.739583\n",
            "Epoch: 161: train_loss/test_loss: 0.302462/0.987207 train_acc/test_acc:0.878977/0.708333\n",
            "Epoch: 162: train_loss/test_loss: 0.357375/0.990825 train_acc/test_acc:0.853977/0.723958\n",
            "Epoch: 163: train_loss/test_loss: 0.349532/0.893861 train_acc/test_acc:0.850568/0.677083\n",
            "Epoch: 164: train_loss/test_loss: 0.372145/0.836203 train_acc/test_acc:0.881818/0.739583\n",
            "Epoch: 165: train_loss/test_loss: 0.356556/0.985320 train_acc/test_acc:0.851136/0.677083\n",
            "Epoch: 166: train_loss/test_loss: 0.392792/1.088523 train_acc/test_acc:0.838636/0.708333\n",
            "Epoch: 167: train_loss/test_loss: 0.332635/0.899423 train_acc/test_acc:0.863068/0.692708\n",
            "Epoch: 168: train_loss/test_loss: 0.335262/0.918589 train_acc/test_acc:0.844318/0.708333\n",
            "Epoch: 169: train_loss/test_loss: 0.353895/0.917514 train_acc/test_acc:0.850568/0.708333\n",
            "Epoch: 170: train_loss/test_loss: 0.396871/0.914475 train_acc/test_acc:0.829545/0.692708\n",
            "Epoch: 171: train_loss/test_loss: 0.425825/0.806360 train_acc/test_acc:0.795455/0.739583\n",
            "Epoch: 172: train_loss/test_loss: 0.316440/0.821128 train_acc/test_acc:0.865909/0.593750\n",
            "Epoch: 173: train_loss/test_loss: 0.371042/0.905187 train_acc/test_acc:0.851136/0.755208\n",
            "Epoch: 174: train_loss/test_loss: 0.348689/0.805262 train_acc/test_acc:0.866477/0.593750\n",
            "Epoch: 175: train_loss/test_loss: 0.307685/0.863969 train_acc/test_acc:0.878977/0.739583\n",
            "Epoch: 176: train_loss/test_loss: 0.339794/0.893250 train_acc/test_acc:0.850568/0.677083\n",
            "Epoch: 177: train_loss/test_loss: 0.366717/0.946639 train_acc/test_acc:0.860795/0.723958\n",
            "Epoch: 178: train_loss/test_loss: 0.345828/0.944887 train_acc/test_acc:0.856818/0.739583\n",
            "Epoch: 179: train_loss/test_loss: 0.348966/0.909584 train_acc/test_acc:0.838068/0.739583\n",
            "Epoch: 180: train_loss/test_loss: 0.332330/0.977738 train_acc/test_acc:0.856818/0.723958\n",
            "Epoch: 181: train_loss/test_loss: 0.363906/1.018650 train_acc/test_acc:0.844886/0.723958\n",
            "Epoch: 182: train_loss/test_loss: 0.342950/0.960561 train_acc/test_acc:0.850568/0.723958\n",
            "Epoch: 183: train_loss/test_loss: 0.317404/0.833731 train_acc/test_acc:0.875568/0.692708\n",
            "Epoch: 184: train_loss/test_loss: 0.313448/0.965310 train_acc/test_acc:0.853977/0.692708\n",
            "Epoch: 185: train_loss/test_loss: 0.356789/1.072028 train_acc/test_acc:0.860227/0.708333\n",
            "Epoch: 186: train_loss/test_loss: 0.329329/0.981136 train_acc/test_acc:0.867045/0.739583\n",
            "Epoch: 187: train_loss/test_loss: 0.293090/0.923813 train_acc/test_acc:0.860227/0.593750\n",
            "Epoch: 188: train_loss/test_loss: 0.349822/0.954601 train_acc/test_acc:0.832386/0.692708\n",
            "Epoch: 189: train_loss/test_loss: 0.342468/0.829555 train_acc/test_acc:0.851136/0.739583\n",
            "Epoch: 190: train_loss/test_loss: 0.313949/0.892693 train_acc/test_acc:0.848295/0.692708\n",
            "Epoch: 191: train_loss/test_loss: 0.287971/1.007268 train_acc/test_acc:0.875568/0.677083\n",
            "Epoch: 192: train_loss/test_loss: 0.286630/1.089409 train_acc/test_acc:0.884659/0.708333\n",
            "Epoch: 193: train_loss/test_loss: 0.298727/1.062318 train_acc/test_acc:0.847727/0.723958\n",
            "Epoch: 194: train_loss/test_loss: 0.272463/0.956732 train_acc/test_acc:0.900000/0.593750\n",
            "Epoch: 195: train_loss/test_loss: 0.292422/1.120022 train_acc/test_acc:0.867045/0.739583\n",
            "Epoch: 196: train_loss/test_loss: 0.334190/0.855529 train_acc/test_acc:0.848295/0.578125\n",
            "Epoch: 197: train_loss/test_loss: 0.366131/0.892930 train_acc/test_acc:0.872159/0.739583\n",
            "Epoch: 198: train_loss/test_loss: 0.292276/1.095479 train_acc/test_acc:0.897159/0.708333\n",
            "Epoch: 199: train_loss/test_loss: 0.349373/1.025207 train_acc/test_acc:0.850568/0.723958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#improved model with a simple skip-connection to the GNN layer to perserve the central node info\n",
        "\n",
        "from torch_geometric.nn import GraphConv\n",
        "from torch_geometric.nn.pool import global_mean_pool\n",
        "\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GNN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GraphConv(in_channels=dataset.num_features, out_channels=hidden_channels)\n",
        "        self.conv2 =  GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
        "        self.conv3 =  GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model_2 = GNN(hidden_channels=64)\n",
        "print(model_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6nntRF0Wpq2",
        "outputId": "3fd98415-acb2-479a-ac61-70bb382bd96b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GNN(\n",
            "  (conv1): GraphConv(7, 64)\n",
            "  (conv2): GraphConv(64, 64)\n",
            "  (conv3): GraphConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_2.parameters(), lr =0.01)\n",
        "\n",
        "for epoch in range(1, 200):\n",
        "  train_loss, train_acc=train(model_2, train_dataloader, loss_fn=loss_fn, optimizer=optimizer)\n",
        "  test_loss, test_acc = test(model_2, test_dataloader, loss_fn= loss_fn)\n",
        "  print(f'Epoch: {epoch}: train_loss/test_loss: {train_loss:2f}/{test_loss:2f} train_acc/test_acc:{train_acc:2f}/{test_acc:2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "ctBPXKjre6l6",
        "outputId": "f4f21150-c6ad-4590-b7b6-96feee2243b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1: train_loss/test_loss: 0.193233/1.273976 train_acc/test_acc:0.934659/0.671875\n",
            "Epoch: 2: train_loss/test_loss: 0.141264/0.617786 train_acc/test_acc:0.940909/0.687500\n",
            "Epoch: 3: train_loss/test_loss: 0.123949/1.091620 train_acc/test_acc:0.956250/0.770833\n",
            "Epoch: 4: train_loss/test_loss: 0.094563/1.092825 train_acc/test_acc:0.968750/0.770833\n",
            "Epoch: 5: train_loss/test_loss: 0.143405/0.914275 train_acc/test_acc:0.940909/0.770833\n",
            "Epoch: 6: train_loss/test_loss: 0.086304/1.140306 train_acc/test_acc:0.968750/0.770833\n",
            "Epoch: 7: train_loss/test_loss: 0.084938/0.685625 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 8: train_loss/test_loss: 0.103977/1.363149 train_acc/test_acc:0.943750/0.786458\n",
            "Epoch: 9: train_loss/test_loss: 0.092716/1.124108 train_acc/test_acc:0.956818/0.786458\n",
            "Epoch: 10: train_loss/test_loss: 0.088901/1.093657 train_acc/test_acc:0.938068/0.703125\n",
            "Epoch: 11: train_loss/test_loss: 0.071972/1.097287 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 12: train_loss/test_loss: 0.060015/1.230582 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 13: train_loss/test_loss: 0.071512/1.492862 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 14: train_loss/test_loss: 0.069950/1.435255 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 15: train_loss/test_loss: 0.076261/1.471726 train_acc/test_acc:0.963068/0.703125\n",
            "Epoch: 16: train_loss/test_loss: 0.073793/1.559828 train_acc/test_acc:0.959659/0.718750\n",
            "Epoch: 17: train_loss/test_loss: 0.113084/1.398941 train_acc/test_acc:0.953409/0.703125\n",
            "Epoch: 18: train_loss/test_loss: 0.111025/1.515527 train_acc/test_acc:0.956250/0.687500\n",
            "Epoch: 19: train_loss/test_loss: 0.102194/1.270327 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 20: train_loss/test_loss: 0.117423/1.286870 train_acc/test_acc:0.940909/0.703125\n",
            "Epoch: 21: train_loss/test_loss: 0.098007/1.350512 train_acc/test_acc:0.968750/0.687500\n",
            "Epoch: 22: train_loss/test_loss: 0.102242/1.124421 train_acc/test_acc:0.956818/0.703125\n",
            "Epoch: 23: train_loss/test_loss: 0.079235/1.387584 train_acc/test_acc:0.968750/0.687500\n",
            "Epoch: 24: train_loss/test_loss: 0.069481/1.320215 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 25: train_loss/test_loss: 0.065639/1.439842 train_acc/test_acc:0.953977/0.703125\n",
            "Epoch: 26: train_loss/test_loss: 0.063998/1.631726 train_acc/test_acc:0.959659/0.718750\n",
            "Epoch: 27: train_loss/test_loss: 0.063225/1.665243 train_acc/test_acc:0.981250/0.718750\n",
            "Epoch: 28: train_loss/test_loss: 0.054818/1.543412 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 29: train_loss/test_loss: 0.076899/1.589949 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 30: train_loss/test_loss: 0.108447/1.642471 train_acc/test_acc:0.947727/0.718750\n",
            "Epoch: 31: train_loss/test_loss: 0.063463/1.420679 train_acc/test_acc:0.968750/0.604167\n",
            "Epoch: 32: train_loss/test_loss: 0.094645/1.829551 train_acc/test_acc:0.959659/0.687500\n",
            "Epoch: 33: train_loss/test_loss: 0.100116/1.452874 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 34: train_loss/test_loss: 0.061425/1.398387 train_acc/test_acc:0.968750/0.718750\n",
            "Epoch: 35: train_loss/test_loss: 0.070062/1.537234 train_acc/test_acc:0.953977/0.703125\n",
            "Epoch: 36: train_loss/test_loss: 0.071126/1.435396 train_acc/test_acc:0.947159/0.703125\n",
            "Epoch: 37: train_loss/test_loss: 0.057406/1.510838 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 38: train_loss/test_loss: 0.068345/1.649732 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 39: train_loss/test_loss: 0.056970/1.493963 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 40: train_loss/test_loss: 0.050356/1.654582 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 41: train_loss/test_loss: 0.055537/1.648812 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 42: train_loss/test_loss: 0.048889/1.789840 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 43: train_loss/test_loss: 0.044735/1.855769 train_acc/test_acc:0.981250/0.718750\n",
            "Epoch: 44: train_loss/test_loss: 0.057641/1.908303 train_acc/test_acc:0.978409/0.718750\n",
            "Epoch: 45: train_loss/test_loss: 0.060985/1.887625 train_acc/test_acc:0.956818/0.703125\n",
            "Epoch: 46: train_loss/test_loss: 0.064200/2.006305 train_acc/test_acc:0.962500/0.718750\n",
            "Epoch: 47: train_loss/test_loss: 0.054953/2.055152 train_acc/test_acc:0.984659/0.703125\n",
            "Epoch: 48: train_loss/test_loss: 0.062450/1.839959 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 49: train_loss/test_loss: 0.082597/1.885392 train_acc/test_acc:0.950568/0.703125\n",
            "Epoch: 50: train_loss/test_loss: 0.069451/2.076343 train_acc/test_acc:0.968750/0.770833\n",
            "Epoch: 51: train_loss/test_loss: 0.051878/1.760188 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 52: train_loss/test_loss: 0.081881/1.903458 train_acc/test_acc:0.950568/0.786458\n",
            "Epoch: 53: train_loss/test_loss: 0.058526/1.832643 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 54: train_loss/test_loss: 0.050962/1.711245 train_acc/test_acc:0.981250/0.687500\n",
            "Epoch: 55: train_loss/test_loss: 0.100133/1.714379 train_acc/test_acc:0.941477/0.687500\n",
            "Epoch: 56: train_loss/test_loss: 0.054823/1.883803 train_acc/test_acc:0.981250/0.770833\n",
            "Epoch: 57: train_loss/test_loss: 0.069882/1.639473 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 58: train_loss/test_loss: 0.049923/1.685995 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 59: train_loss/test_loss: 0.070663/1.797512 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 60: train_loss/test_loss: 0.064950/1.766136 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 61: train_loss/test_loss: 0.060566/1.918545 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 62: train_loss/test_loss: 0.062112/1.810400 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 63: train_loss/test_loss: 0.109476/1.819778 train_acc/test_acc:0.953977/0.703125\n",
            "Epoch: 64: train_loss/test_loss: 0.115179/1.438164 train_acc/test_acc:0.950000/0.619792\n",
            "Epoch: 65: train_loss/test_loss: 0.167099/1.154768 train_acc/test_acc:0.922159/0.687500\n",
            "Epoch: 66: train_loss/test_loss: 0.098457/1.190261 train_acc/test_acc:0.962500/0.786458\n",
            "Epoch: 67: train_loss/test_loss: 0.074494/1.459845 train_acc/test_acc:0.950568/0.770833\n",
            "Epoch: 68: train_loss/test_loss: 0.076123/1.123439 train_acc/test_acc:0.968750/0.687500\n",
            "Epoch: 69: train_loss/test_loss: 0.100593/1.190385 train_acc/test_acc:0.950568/0.687500\n",
            "Epoch: 70: train_loss/test_loss: 0.067164/1.815930 train_acc/test_acc:0.968750/0.755208\n",
            "Epoch: 71: train_loss/test_loss: 0.111713/1.285634 train_acc/test_acc:0.950568/0.703125\n",
            "Epoch: 72: train_loss/test_loss: 0.076229/1.502860 train_acc/test_acc:0.947159/0.703125\n",
            "Epoch: 73: train_loss/test_loss: 0.062034/1.664173 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 74: train_loss/test_loss: 0.083946/1.717718 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 75: train_loss/test_loss: 0.062556/1.643292 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 76: train_loss/test_loss: 0.069649/1.906173 train_acc/test_acc:0.962500/0.703125\n",
            "Epoch: 77: train_loss/test_loss: 0.065815/1.814466 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 78: train_loss/test_loss: 0.056264/1.747215 train_acc/test_acc:0.978409/0.619792\n",
            "Epoch: 79: train_loss/test_loss: 0.081693/1.794777 train_acc/test_acc:0.953409/0.718750\n",
            "Epoch: 80: train_loss/test_loss: 0.097318/1.927875 train_acc/test_acc:0.953977/0.786458\n",
            "Epoch: 81: train_loss/test_loss: 0.062530/1.728231 train_acc/test_acc:0.975000/0.619792\n",
            "Epoch: 82: train_loss/test_loss: 0.091606/2.084028 train_acc/test_acc:0.953409/0.770833\n",
            "Epoch: 83: train_loss/test_loss: 0.118394/1.568470 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 84: train_loss/test_loss: 0.093160/1.455260 train_acc/test_acc:0.950000/0.703125\n",
            "Epoch: 85: train_loss/test_loss: 0.098788/1.535509 train_acc/test_acc:0.953409/0.770833\n",
            "Epoch: 86: train_loss/test_loss: 0.080176/1.400195 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 87: train_loss/test_loss: 0.054965/1.416041 train_acc/test_acc:0.972159/0.786458\n",
            "Epoch: 88: train_loss/test_loss: 0.061825/1.298833 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 89: train_loss/test_loss: 0.053817/1.768612 train_acc/test_acc:0.981250/0.786458\n",
            "Epoch: 90: train_loss/test_loss: 0.076478/1.754060 train_acc/test_acc:0.966477/0.786458\n",
            "Epoch: 91: train_loss/test_loss: 0.081925/1.500223 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 92: train_loss/test_loss: 0.056561/1.732794 train_acc/test_acc:0.981250/0.687500\n",
            "Epoch: 93: train_loss/test_loss: 0.075824/1.599045 train_acc/test_acc:0.950568/0.703125\n",
            "Epoch: 94: train_loss/test_loss: 0.069737/1.601428 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 95: train_loss/test_loss: 0.054130/1.598491 train_acc/test_acc:0.962500/0.786458\n",
            "Epoch: 96: train_loss/test_loss: 0.062990/1.922765 train_acc/test_acc:0.975000/0.786458\n",
            "Epoch: 97: train_loss/test_loss: 0.055211/1.824179 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 98: train_loss/test_loss: 0.050337/1.756701 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 99: train_loss/test_loss: 0.046602/1.985694 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 100: train_loss/test_loss: 0.056633/2.174271 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 101: train_loss/test_loss: 0.049921/2.097417 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 102: train_loss/test_loss: 0.053637/2.368608 train_acc/test_acc:0.981250/0.786458\n",
            "Epoch: 103: train_loss/test_loss: 0.065723/2.405279 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 104: train_loss/test_loss: 0.071896/2.378607 train_acc/test_acc:0.975000/0.786458\n",
            "Epoch: 105: train_loss/test_loss: 0.067465/1.942218 train_acc/test_acc:0.962500/0.703125\n",
            "Epoch: 106: train_loss/test_loss: 0.095281/2.282061 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 107: train_loss/test_loss: 0.065436/1.905253 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 108: train_loss/test_loss: 0.061410/1.940770 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 109: train_loss/test_loss: 0.054078/1.794963 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 110: train_loss/test_loss: 0.060877/1.916326 train_acc/test_acc:0.968750/0.786458\n",
            "Epoch: 111: train_loss/test_loss: 0.042870/2.169072 train_acc/test_acc:0.981250/0.786458\n",
            "Epoch: 112: train_loss/test_loss: 0.056777/2.066851 train_acc/test_acc:0.975568/0.802083\n",
            "Epoch: 113: train_loss/test_loss: 0.075968/1.796190 train_acc/test_acc:0.953409/0.703125\n",
            "Epoch: 114: train_loss/test_loss: 0.078232/1.925522 train_acc/test_acc:0.947159/0.718750\n",
            "Epoch: 115: train_loss/test_loss: 0.043528/1.845017 train_acc/test_acc:0.981250/0.718750\n",
            "Epoch: 116: train_loss/test_loss: 0.045994/1.874766 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 117: train_loss/test_loss: 0.043558/2.048547 train_acc/test_acc:0.981250/0.786458\n",
            "Epoch: 118: train_loss/test_loss: 0.047773/2.119964 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 119: train_loss/test_loss: 0.055055/2.227365 train_acc/test_acc:0.963068/0.786458\n",
            "Epoch: 120: train_loss/test_loss: 0.065448/2.203762 train_acc/test_acc:0.972159/0.786458\n",
            "Epoch: 121: train_loss/test_loss: 0.055603/2.050930 train_acc/test_acc:0.962500/0.786458\n",
            "Epoch: 122: train_loss/test_loss: 0.068852/2.153806 train_acc/test_acc:0.975000/0.802083\n",
            "Epoch: 123: train_loss/test_loss: 0.058623/1.953797 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 124: train_loss/test_loss: 0.062890/2.026998 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 125: train_loss/test_loss: 0.040544/2.047838 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 126: train_loss/test_loss: 0.042050/2.209687 train_acc/test_acc:0.984659/0.786458\n",
            "Epoch: 127: train_loss/test_loss: 0.045039/2.226189 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 128: train_loss/test_loss: 0.056532/2.339097 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 129: train_loss/test_loss: 0.062627/2.527982 train_acc/test_acc:0.975000/0.786458\n",
            "Epoch: 130: train_loss/test_loss: 0.049554/2.095466 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 131: train_loss/test_loss: 0.049973/2.256132 train_acc/test_acc:0.972159/0.802083\n",
            "Epoch: 132: train_loss/test_loss: 0.051553/2.453427 train_acc/test_acc:0.972159/0.786458\n",
            "Epoch: 133: train_loss/test_loss: 0.043001/2.206153 train_acc/test_acc:0.987500/0.786458\n",
            "Epoch: 134: train_loss/test_loss: 0.052209/2.334720 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 135: train_loss/test_loss: 0.069376/2.468934 train_acc/test_acc:0.968750/0.786458\n",
            "Epoch: 136: train_loss/test_loss: 0.074400/2.163819 train_acc/test_acc:0.963068/0.786458\n",
            "Epoch: 137: train_loss/test_loss: 0.070222/1.995238 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 138: train_loss/test_loss: 0.042718/2.206466 train_acc/test_acc:0.975000/0.786458\n",
            "Epoch: 139: train_loss/test_loss: 0.059018/2.137340 train_acc/test_acc:0.965909/0.802083\n",
            "Epoch: 140: train_loss/test_loss: 0.062770/2.133743 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 141: train_loss/test_loss: 0.054987/2.238271 train_acc/test_acc:0.972159/0.786458\n",
            "Epoch: 142: train_loss/test_loss: 0.046030/2.180842 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 143: train_loss/test_loss: 0.051092/2.077667 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 144: train_loss/test_loss: 0.064948/2.090315 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 145: train_loss/test_loss: 0.049846/2.362385 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 146: train_loss/test_loss: 0.039405/2.110464 train_acc/test_acc:0.978409/0.786458\n",
            "Epoch: 147: train_loss/test_loss: 0.055742/2.277369 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 148: train_loss/test_loss: 0.078602/2.545784 train_acc/test_acc:0.963068/0.786458\n",
            "Epoch: 149: train_loss/test_loss: 0.065749/2.077867 train_acc/test_acc:0.956818/0.786458\n",
            "Epoch: 150: train_loss/test_loss: 0.039673/2.151428 train_acc/test_acc:0.975000/0.786458\n",
            "Epoch: 151: train_loss/test_loss: 0.059502/2.022057 train_acc/test_acc:0.981250/0.687500\n",
            "Epoch: 152: train_loss/test_loss: 0.086351/2.241508 train_acc/test_acc:0.953409/0.786458\n",
            "Epoch: 153: train_loss/test_loss: 0.079341/2.292058 train_acc/test_acc:0.965909/0.770833\n",
            "Epoch: 154: train_loss/test_loss: 0.098005/2.105550 train_acc/test_acc:0.956250/0.755208\n",
            "Epoch: 155: train_loss/test_loss: 0.107893/2.901594 train_acc/test_acc:0.959659/0.786458\n",
            "Epoch: 156: train_loss/test_loss: 0.145785/1.820180 train_acc/test_acc:0.947159/0.687500\n",
            "Epoch: 157: train_loss/test_loss: 0.088165/1.771378 train_acc/test_acc:0.940909/0.671875\n",
            "Epoch: 158: train_loss/test_loss: 0.085385/1.658110 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 159: train_loss/test_loss: 0.090279/1.593485 train_acc/test_acc:0.947727/0.619792\n",
            "Epoch: 160: train_loss/test_loss: 0.099231/1.775958 train_acc/test_acc:0.953409/0.703125\n",
            "Epoch: 161: train_loss/test_loss: 0.060343/1.630296 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 162: train_loss/test_loss: 0.087941/1.624992 train_acc/test_acc:0.947159/0.703125\n",
            "Epoch: 163: train_loss/test_loss: 0.056449/1.706833 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 164: train_loss/test_loss: 0.054941/1.736094 train_acc/test_acc:0.972159/0.687500\n",
            "Epoch: 165: train_loss/test_loss: 0.056125/1.726586 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 166: train_loss/test_loss: 0.050430/1.873495 train_acc/test_acc:0.968750/0.687500\n",
            "Epoch: 167: train_loss/test_loss: 0.068956/2.008588 train_acc/test_acc:0.972159/0.687500\n",
            "Epoch: 168: train_loss/test_loss: 0.050725/1.928322 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 169: train_loss/test_loss: 0.081617/1.975912 train_acc/test_acc:0.950568/0.703125\n",
            "Epoch: 170: train_loss/test_loss: 0.046196/2.217532 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 171: train_loss/test_loss: 0.054234/2.062335 train_acc/test_acc:0.978409/0.703125\n",
            "Epoch: 172: train_loss/test_loss: 0.048898/2.080229 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 173: train_loss/test_loss: 0.043738/2.241817 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 174: train_loss/test_loss: 0.051554/2.207802 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 175: train_loss/test_loss: 0.064155/2.215967 train_acc/test_acc:0.963068/0.703125\n",
            "Epoch: 176: train_loss/test_loss: 0.057722/2.401292 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 177: train_loss/test_loss: 0.056969/2.301845 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 178: train_loss/test_loss: 0.066197/2.233448 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 179: train_loss/test_loss: 0.076009/2.197249 train_acc/test_acc:0.950568/0.703125\n",
            "Epoch: 180: train_loss/test_loss: 0.057995/2.250297 train_acc/test_acc:0.965909/0.718750\n",
            "Epoch: 181: train_loss/test_loss: 0.093369/2.333261 train_acc/test_acc:0.975568/0.786458\n",
            "Epoch: 182: train_loss/test_loss: 0.049973/1.975281 train_acc/test_acc:0.965909/0.619792\n",
            "Epoch: 183: train_loss/test_loss: 0.087347/2.034041 train_acc/test_acc:0.962500/0.786458\n",
            "Epoch: 184: train_loss/test_loss: 0.042093/2.457604 train_acc/test_acc:0.981250/0.770833\n",
            "Epoch: 185: train_loss/test_loss: 0.052728/1.941942 train_acc/test_acc:0.981250/0.786458\n",
            "Epoch: 186: train_loss/test_loss: 0.083875/1.836699 train_acc/test_acc:0.950568/0.619792\n",
            "Epoch: 187: train_loss/test_loss: 0.069489/2.358236 train_acc/test_acc:0.956818/0.770833\n",
            "Epoch: 188: train_loss/test_loss: 0.077959/1.822929 train_acc/test_acc:0.965909/0.786458\n",
            "Epoch: 189: train_loss/test_loss: 0.066763/1.775588 train_acc/test_acc:0.959659/0.703125\n",
            "Epoch: 190: train_loss/test_loss: 0.047139/2.024085 train_acc/test_acc:0.975000/0.770833\n",
            "Epoch: 191: train_loss/test_loss: 0.074559/1.900593 train_acc/test_acc:0.981250/0.718750\n",
            "Epoch: 192: train_loss/test_loss: 0.049708/1.841828 train_acc/test_acc:0.975000/0.703125\n",
            "Epoch: 193: train_loss/test_loss: 0.051436/1.939695 train_acc/test_acc:0.962500/0.786458\n",
            "Epoch: 194: train_loss/test_loss: 0.047125/2.024817 train_acc/test_acc:0.968750/0.703125\n",
            "Epoch: 195: train_loss/test_loss: 0.055300/2.152606 train_acc/test_acc:0.981250/0.703125\n",
            "Epoch: 196: train_loss/test_loss: 0.050411/2.089029 train_acc/test_acc:0.972159/0.703125\n",
            "Epoch: 197: train_loss/test_loss: 0.052737/2.179435 train_acc/test_acc:0.969318/0.703125\n",
            "Epoch: 198: train_loss/test_loss: 0.047716/2.074585 train_acc/test_acc:0.965909/0.703125\n",
            "Epoch: 199: train_loss/test_loss: 0.038073/2.301670 train_acc/test_acc:0.975000/0.786458\n"
          ]
        }
      ]
    }
  ]
}